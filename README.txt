For this assignment, I have learned that preprocessing data is an important step for machine learning, and that depending on the quality of the data source, the challenge and the importance of data preprocessing becomes even greater. As many data scientists know, clean kaggle datasets are not representative of the real world, and often one will encounter data with challenges which requires intensive feature preprocessing like normalization, imputing, feature selection and data balancing. This variation in datasets has also showed me that there is no “perfect model”. Certain models will indeed perform better than others depending on its architecture, hyper parameter tuning, and the type of data balancing used, which we saw in our results.

In this assignment, I used a total of 6 classifier models which were Decision Trees, Random Forest, K Nearest Neighbour, Support Vector Machines, Multilayer Perceptron and Gradient Boosting. In order to optimize the parameters, I used grid search. The models were passed through a cross validation algorithm of 10 folds. For each dataset, the cross validation algorithm was tested on three versions of the dataset, one was the normal dataset, the other an oversampling of the minority class, and finally an undersampling of the majority class.

As we can see with our results for the drugs dataset with cannabis, we achieve the best accuracy with random forest that has been oversampled with an accuracy of 83%. For the labour dataset, gradient boosting performed the best on the oversampled dataset with an accuracy of 93%. Finally, for the heart dataset, the multilayer perceptron acheived the best accuracy on the undersampled dataset with an accuracy of 86%. 

I also tested my results for the 5 datasets with the friedman test, which shows that there is a significant difference between some of the algorithms. Using the nemenyi test which gave a CD of 3.37, we found that the difference between the average rankings of the algorithms resulted with RF and KNN having a difference above the CD, showing that there is a critical difference between the two. Note that we get the same results if we include the over and undersampling datasets for the heart and labour datasets, resulting in 9 datasets instead of 5.

Please check results.txt for more info on the ranking of the models in terms of accuracy, and the friedman and nemenyi test.
